{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RegistryException",
     "evalue": "Factory with name 'LabelSmoothingLoss' is already present\nAlready registered: '<class '__main__.LabelSmoothingLoss'>'\nNew: '<class '__main__.LabelSmoothingLoss'>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRegistryException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-050907b76735>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCriterion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mLabelSmoothingLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmooth_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catalyst-20.12.1-py3.8.egg\\catalyst\\registry\\registry.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, factory, name, *factories, **named_factories)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[1;31m# https://github.com/catalyst-team/catalyst/issues/135\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_factories\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_factories\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m                 raise RegistryException(\n\u001b[0m\u001b[0;32m    128\u001b[0m                     \u001b[1;34mf\"Factory with name '{name}' is already present\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m                     \u001b[1;34mf\"Already registered: '{self._factories[name]}'\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRegistryException\u001b[0m: Factory with name 'LabelSmoothingLoss' is already present\nAlready registered: '<class '__main__.LabelSmoothingLoss'>'\nNew: '<class '__main__.LabelSmoothingLoss'>'"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from catalyst.contrib import registry\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "def log_t(u, t):\n",
    "    \"\"\"Compute log_t for `u`.\"\"\"\n",
    "\n",
    "    if t == 1.0:\n",
    "        return torch.log(u)\n",
    "    else:\n",
    "        return (u ** (1.0 - t) - 1.0) / (1.0 - t)\n",
    "\n",
    "\n",
    "def exp_t(u, t):\n",
    "    \"\"\"Compute exp_t for `u`.\"\"\"\n",
    "\n",
    "    if t == 1.0:\n",
    "        return torch.exp(u)\n",
    "    else:\n",
    "        return torch.relu(1.0 + (1.0 - t) * u) ** (1.0 / (1.0 - t))\n",
    "\n",
    "\n",
    "def compute_normalization_fixed_point(activations, t, num_iters=5):\n",
    "    \"\"\"Returns the normalization value for each example (t > 1.0).\n",
    "    Args:\n",
    "    activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "    t: Temperature 2 (> 1.0 for tail heaviness).\n",
    "    num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "\n",
    "    mu = torch.max(activations, dim=-1).values.view(-1, 1)\n",
    "    normalized_activations_step_0 = activations - mu\n",
    "\n",
    "    normalized_activations = normalized_activations_step_0\n",
    "    i = 0\n",
    "    while i < num_iters:\n",
    "        i += 1\n",
    "        logt_partition = torch.sum(exp_t(normalized_activations, t), dim=-1).view(-1, 1)\n",
    "        normalized_activations = normalized_activations_step_0 * (logt_partition ** (1.0 - t))\n",
    "\n",
    "    logt_partition = torch.sum(exp_t(normalized_activations, t), dim=-1).view(-1, 1)\n",
    "\n",
    "    return -log_t(1.0 / logt_partition, t) + mu\n",
    "\n",
    "\n",
    "def compute_normalization(activations, t, num_iters=5):\n",
    "    \"\"\"Returns the normalization value for each example.\n",
    "    Args:\n",
    "    activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "    t: Temperature 2 (< 1.0 for finite support, > 1.0 for tail heaviness).\n",
    "    num_iters: Number of iterations to run the method.\n",
    "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
    "    \"\"\"\n",
    "\n",
    "    if t < 1.0:\n",
    "        return None # not implemented as these values do not occur in the authors experiments...\n",
    "    else:\n",
    "        return compute_normalization_fixed_point(activations, t, num_iters)\n",
    "\n",
    "\n",
    "def tempered_softmax(activations, t, num_iters=5):\n",
    "    \"\"\"Tempered softmax function.\n",
    "    Args:\n",
    "    activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "    t: Temperature tensor > 0.0.\n",
    "    num_iters: Number of iterations to run the method.\n",
    "    Returns:\n",
    "    A probabilities tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    if t == 1.0:\n",
    "        normalization_constants = torch.log(torch.sum(torch.exp(activations), dim=-1))\n",
    "    else:\n",
    "        normalization_constants = compute_normalization(activations, t, num_iters)\n",
    "\n",
    "    return exp_t(activations - normalization_constants, t)\n",
    "\n",
    "\n",
    "def bi_tempered_logistic_loss(activations, labels, t1, t2, label_smoothing=0.0, num_iters=5):\n",
    "\n",
    "    \"\"\"Bi-Tempered Logistic Loss with custom gradient.\n",
    "    Args:\n",
    "    activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
    "    labels: A tensor with shape and dtype as activations.\n",
    "    t1: Temperature 1 (< 1.0 for boundedness).\n",
    "    t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
    "    label_smoothing: Label smoothing parameter between [0, 1).\n",
    "    num_iters: Number of iterations to run the method.\n",
    "    Returns:\n",
    "    A loss tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    if label_smoothing > 0.0:\n",
    "        num_classes = labels.shape[-1]\n",
    "        labels = (1 - num_classes / (num_classes - 1) * label_smoothing) * labels + label_smoothing / (num_classes - 1)\n",
    "\n",
    "    probabilities = tempered_softmax(activations, t2, num_iters)\n",
    "\n",
    "    temp1 = (log_t(labels + 1e-10, t1) - log_t(probabilities, t1)) * labels\n",
    "    temp2 = (1 / (2 - t1)) * (torch.pow(labels, 2 - t1) - torch.pow(probabilities, 2 - t1))\n",
    "    loss_values = temp1 - temp2\n",
    "\n",
    "    return torch.sum(loss_values, dim=-1)\n",
    "   \n",
    "    \n",
    "@registry.Criterion\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, smooth_factor=0.05):\n",
    "        super().__init__()\n",
    "        self.smooth_factor = smooth_factor\n",
    "\n",
    "    def _smooth_labels(self, num_classes, target):\n",
    "        # When label smoothing is turned on,\n",
    "        # KL-divergence between q_{smoothed ground truth prob.}(w)\n",
    "        # and p_{prob. computed by model}(w) is minimized.\n",
    "        # If label smoothing value is set to zero, the loss\n",
    "        # is equivalent to NLLLoss or CrossEntropyLoss.\n",
    "        # All non-true labels are uniformly set to low-confidence.\n",
    "\n",
    "        target_one_hot = F.one_hot(target, num_classes).float()\n",
    "        target_one_hot[target_one_hot == 1] = 1 - self.smooth_factor\n",
    "        target_one_hot[target_one_hot == 0] = self.smooth_factor\n",
    "        return target_one_hot\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = F.log_softmax(input, dim=1)\n",
    "        target_one_hot = self._smooth_labels(input.size(1), target)\n",
    "        return F.kl_div(logp, target_one_hot, reduction='sum')\n",
    "\n",
    "@registry.Criterion\n",
    "class TemperedLogLoss(nn.Module):\n",
    "    def __init__(self, label_smoothing=0.05, t1 = 0.7, t2 = 2, num_iters = 5):\n",
    "        super().__init__()\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.t1 = t1\n",
    "        self.t2 = t2\n",
    "        self.num_iters = num_iters\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return bi_tempered_logistic_loss(input, target, self.t1, self.t2, self.label_smoothing, self.num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "\n",
    "def get_super_light_augmentations(image_size):\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "    ])\n",
    "\n",
    "def get_light_augmentations(image_size):\n",
    "    min_size = min(image_size[0], image_size[1])\n",
    "    return A.Compose([\n",
    "        A.RandomSizedCrop(min_max_height=(int(min_size * 0.85), min_size),\n",
    "                          height=image_size[0],\n",
    "                          width=image_size[1], p=1.0),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "    ])\n",
    "\n",
    "def get_medium_augmentations(image_size):\n",
    "    min_size = min(image_size[0], image_size[1])\n",
    "\n",
    "    return A.Compose([\n",
    "        A.OneOf([A.RandomSizedCrop(min_max_height=(int(min_size* 0.85), min_size),\n",
    "                          height=image_size[0],\n",
    "                          width=image_size[1]),\n",
    "                A.Resize(image_size[0], image_size[1]),\n",
    "                A.CenterCrop(image_size[0], image_size[1])\n",
    "                ], p = 1.0),\n",
    "\n",
    "        A.OneOf([\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.15,\n",
    "                                       contrast_limit=0.5),\n",
    "            A.RandomGamma(gamma_limit=(50, 150)),\n",
    "            A.NoOp()\n",
    "        ], p = 1.0),\n",
    "        \n",
    "        A.OneOf([A.CLAHE(p=0.5, clip_limit=(10, 10), tile_grid_size=(3, 3)),\n",
    "                A.FancyPCA(alpha=0.4),\n",
    "                A.NoOp(),\n",
    "                ], p = 1.0),\n",
    "        \n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.Flip(p = 0.5)\n",
    "    ])\n",
    "\n",
    "def get_hard_augmentations(image_size):\n",
    "    return None\n",
    "\n",
    "# def get_medium_augmentations(image_size):\n",
    "#     return A.Compose([\n",
    "#         A.OneOf([\n",
    "#             A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1,\n",
    "#                                rotate_limit=15,\n",
    "#                                border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "#             A.OpticalDistortion(distort_limit=0.11, shift_limit=0.15,\n",
    "#                                 border_mode=cv2.BORDER_CONSTANT,\n",
    "#                                 value=0),\n",
    "#             A.NoOp()\n",
    "#         ]),\n",
    "#         A.RandomSizedCrop(min_max_height=(int(image_size[0] * 0.75), image_size[0]),\n",
    "#                           height=image_size[0],\n",
    "#                           width=image_size[1], p=0.3),\n",
    "#         A.OneOf([\n",
    "#             A.RandomBrightnessContrast(brightness_limit=0.5,\n",
    "#                                        contrast_limit=0.4),\n",
    "#             A.RandomGamma(gamma_limit=(50, 150)),\n",
    "#             A.NoOp()\n",
    "#         ]),\n",
    "#         A.OneOf([\n",
    "#             A.RGBShift(r_shift_limit=20, b_shift_limit=15, g_shift_limit=15),\n",
    "#             A.HueSaturationValue(hue_shift_limit=5,\n",
    "#                                  sat_shift_limit=5),\n",
    "#             A.NoOp()\n",
    "#         ]),\n",
    "#         A.HorizontalFlip(p=0.5),\n",
    "#         A.VerticalFlip(p=0.5)\n",
    "#     ])\n",
    "\n",
    "def post_transforms():\n",
    "    # we use ImageNet image normalization\n",
    "    # and convert it to torch.Tensor\n",
    "#     return [ToTensor()]\n",
    "    return A.Compose([A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                       std=(0.229, 0.224, 0.225)), ToTensor()])\n",
    "\n",
    "# def compose(transforms_to_compose):\n",
    "#     # combine all augmentations into one single pipeline\n",
    "#     result = A.Compose([\n",
    "#       item for sublist in transforms_to_compose for item in sublist\n",
    "#     ])\n",
    "#     return result\n",
    "\n",
    "\n",
    "def get_test_transform(image_size):\n",
    "    return A.Compose([A.Resize(image_size[0], image_size[1], p = 1.0), A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                       std=(0.229, 0.224, 0.225), p = 1.0), ToTensor()])\n",
    "\n",
    "def get_train_transform(augmentation, image_size):\n",
    "    LEVELS = {\n",
    "        'super_light': get_super_light_augmentations,\n",
    "        'light': get_light_augmentations,\n",
    "        'medium': get_medium_augmentations,\n",
    "        'hard': get_hard_augmentations,\n",
    "#         'hard2': get_hard_augmentations_v2\n",
    "    }\n",
    "\n",
    "    aug = LEVELS[augmentation](image_size)\n",
    "\n",
    "    return A.Compose([aug, post_transforms()])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from pytorch_toolbelt.utils import fs\n",
    "# from pytorch_toolbelt.utils.fs import id_from_fname\n",
    "# from pytorch_toolbelt.utils.torch_utils import tensor_from_rgb_image\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.utils import compute_sample_weight\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "\n",
    "\n",
    "def prepare_by_stretch_img(img, img_size):\n",
    "    shape = (img.shape[1], img.shape[2])\n",
    "\n",
    "    if shape[0] < img_size[0] or shape[1] < img_size[1]:\n",
    "        coef_0 = float(img_size[0]) / shape[0]\n",
    "        coef_1 = float(img_size[1]) / shape[1]\n",
    "\n",
    "        coef = int(max(coef_0, coef_1)) + 1\n",
    "\n",
    "        return cv2.resize(img, (shape[0]*coef, shape[1]*coef))\n",
    "        \n",
    "    return img\n",
    "\n",
    "def prepare_by_reshape(img, img_size):\n",
    "    return cv2.resize(img, (img_size[0], img_size[1]))\n",
    "\n",
    "UNLABELED_CLASS = -100\n",
    "\n",
    "def get_prepare_function(prepare_function_name):\n",
    "    d = {\n",
    "        'reshape': prepare_by_reshape,\n",
    "        'stretch': prepare_by_stretch_img,\n",
    "    }\n",
    "    return d[prepare_function_name]\n",
    "\n",
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, images, targets, img_size, prepare_function_name,\n",
    "                 transform: A.Compose,\n",
    "                 target_as_array=False,\n",
    "                 dtype=int):\n",
    "        if targets is not None:\n",
    "            targets = np.array(targets)\n",
    "            unique_targets = set(targets)\n",
    "            if len(unique_targets.difference({0, 1, 2, 3, 4, UNLABELED_CLASS})):\n",
    "                raise ValueError('Unexpected targets in Y ' + str(unique_targets))\n",
    "\n",
    "        self.images = np.array(images)\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "        self.target_as_array = target_as_array\n",
    "        self.dtype = dtype\n",
    "        self.img_size = img_size\n",
    "        self.prepare_function_name = prepare_function_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image = cv2.imread(self.images[item])  # Read with OpenCV instead PIL. It's faster\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(self.images[item])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        prep = get_prepare_function(self.prepare_function_name)\n",
    "        image = prep(image, self.img_size)\n",
    "        height, width = image.shape[:2]\n",
    "        label = UNLABELED_CLASS\n",
    "\n",
    "        if self.targets is not None:\n",
    "            label = self.targets[item]\n",
    "\n",
    "        #Плохо тут это делать но пока так\n",
    "        # image = prepare_img(image, self.img_size)\n",
    "        data = self.transform(image=image, label=label)\n",
    "        label = data['label']\n",
    "\n",
    "        data = {'image': data['image']}\n",
    "\n",
    "        label = self.dtype(label)\n",
    "        if self.target_as_array:\n",
    "            data['targets'] = np.array([label])\n",
    "        else:\n",
    "            data['targets'] = label\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_train_valid(x, y, fold=None, folds=4, random_state=42):\n",
    "    \"\"\"\n",
    "    Common train/test split function\n",
    "    :param x:\n",
    "    :param y:\n",
    "    :param fold:\n",
    "    :param folds:\n",
    "    :param random_state:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_x, train_y = [], []\n",
    "    valid_x, valid_y = [], []\n",
    "\n",
    "    if fold is not None:\n",
    "        assert 0 <= fold < folds\n",
    "        skf = StratifiedKFold(n_splits=folds, random_state=random_state, shuffle=True)\n",
    "\n",
    "        for fold_index, (train_index, test_index) in enumerate(skf.split(x, y)):\n",
    "            if fold_index == fold:\n",
    "                train_x = x[train_index]\n",
    "                train_y = y[train_index]\n",
    "                valid_x = x[test_index]\n",
    "                valid_y = y[test_index]\n",
    "                break\n",
    "    else:\n",
    "        train_x, valid_x, train_y, valid_y = train_test_split(x, y,\n",
    "                                                              random_state=random_state,\n",
    "                                                              test_size=1.0 / folds,\n",
    "                                                              shuffle=True,\n",
    "                                                              stratify=y)\n",
    "\n",
    "    assert len(train_x) and len(train_y) and len(valid_x) and len(valid_y)\n",
    "    assert len(train_x) == len(train_y)\n",
    "    assert len(valid_x) == len(valid_y)\n",
    "    return train_x, valid_x, train_y, valid_y\n",
    "\n",
    "\n",
    "def get_current_train(data_dir):\n",
    "    df = pd.read_csv(os.path.join(data_dir, 'train_images.csv'))\n",
    "    x = np.array(df['image_id'].apply(lambda x: os.path.join(data_dir, 'train_images', f'{x}')))\n",
    "    y = np.array(df['label'], dtype=int)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_extra_train(data_dir):\n",
    "    df = pd.read_csv(os.path.join(data_dir, 'extra_images.csv'))\n",
    "    x = np.array(df['id_code'].apply(lambda x: os.path.join(data_dir, 'extra_data', f'{x}')))\n",
    "    y = np.array(df['label'], dtype=int)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_unlabeled(dataset_dir, healthy_eye_fraction=1):\n",
    "    df= pd.read_csv(os.path.join(dataset_dir, 'unlabeled.csv'))\n",
    "    x = np.array(df['image_id'].apply(lambda x: os.path.join(data_dir, 'unlabeled', f'{x}')))\n",
    "    y = np.array([UNLABELED_CLASS] * len(x), dtype=int)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def append_train_test(existing, to_add):\n",
    "    train_x, train_y, valid_x, valid_y = existing\n",
    "    tx, vx, ty, vy = to_add\n",
    "    train_x.extend(tx)\n",
    "    train_y.extend(ty)\n",
    "    valid_x.extend(vx)\n",
    "    valid_y.extend(vy)\n",
    "    return train_x, train_y, valid_x, valid_y\n",
    "\n",
    "\n",
    "def get_dataset(datasets: List[str], data_dir='data', random_state=42):\n",
    "    \"\"\"\n",
    "    :param datasets: List \"aptos2015-train/fold0\", \"messidor\",\n",
    "    :param fold:\n",
    "    :param folds:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    all_x = []\n",
    "    all_y = []\n",
    "    sizes = []\n",
    "\n",
    "    for ds in datasets:\n",
    "\n",
    "        dataset_name = ds\n",
    "\n",
    "        if dataset_name == 'current_train':\n",
    "            x, y = get_current_train(data_dir)\n",
    "        elif dataset_name == 'extra_train':\n",
    "            x, y = get_extra_train(data_dir)\n",
    "        elif dataset_name == 'unlabeled':\n",
    "            x, y = get_aptos2015_test(data_dir)\n",
    "        else:\n",
    "            raise ValueError(dataset_name)\n",
    "\n",
    "        all_x.extend(x)\n",
    "        all_y.extend(y)\n",
    "        sizes.append(len(x))\n",
    "\n",
    "    return all_x, all_y, sizes\n",
    "\n",
    "\n",
    "def get_datasets_universal(\n",
    "        train_on: List[str],\n",
    "        valid_on: List[str],\n",
    "        data_dir='data',\n",
    "        prep_function = 'reshape',\n",
    "        image_size=(512, 512),\n",
    "        augmentation='medium',\n",
    "        preprocessing=None,\n",
    "        target_dtype=int,\n",
    "        random_state=42,\n",
    "        coarse_grading=False,\n",
    "        folds=4) -> Tuple[TaskDataset, TaskDataset, List]:\n",
    "    train_x, train_y, sizes = get_dataset(train_on, data_dir=data_dir, random_state=random_state)\n",
    "    valid_x, valid_y, _ = get_dataset(valid_on, data_dir=data_dir, random_state=random_state)\n",
    "\n",
    "    train_transform = get_train_transform(augmentation = augmentation, image_size = image_size)\n",
    "    valid_transform = get_test_transform()\n",
    "\n",
    "    train_ds = TaskDataset(train_x, train_y, image_size, prep_function,\n",
    "                                  transform=train_transform,\n",
    "                                  dtype=target_dtype)\n",
    "\n",
    "    valid_ds = TaskDataset(valid_x, valid_y, image_size, prep_function,\n",
    "                                  transform=valid_transform,\n",
    "                                  dtype=target_dtype)\n",
    "\n",
    "    return train_ds, valid_ds, sizes\n",
    "\n",
    "\n",
    "def get_datasets(\n",
    "        data_dir='data',\n",
    "        prep_function = 'reshape',\n",
    "        image_size=(512, 512),\n",
    "        augmentation='medium',\n",
    "        use_current=True,\n",
    "        use_extra=False,\n",
    "        use_unlabeled=False,\n",
    "        target_dtype=int,\n",
    "        random_state=42,\n",
    "        fold=None,\n",
    "        folds=4) -> Tuple[TaskDataset, TaskDataset, List]:\n",
    "    assert use_current or use_extra or use_unlabeled\n",
    "\n",
    "    trainset_sizes = []\n",
    "    data_split = [], [], [], []\n",
    "\n",
    "  \n",
    "\n",
    "    if use_current:\n",
    "        x, y = get_current_train(data_dir)\n",
    "        split = split_train_valid(x, y, fold=fold, folds=folds, random_state=random_state)\n",
    "        data_split = append_train_test(data_split, split)\n",
    "        trainset_sizes.append(len(split[0]))\n",
    "\n",
    "    if use_extra:\n",
    "        x, y = get_extra_train(data_dir)\n",
    "        split = split_train_valid(x, y, fold=fold, folds=folds, random_state=random_state)\n",
    "        data_split = append_train_test(data_split, split)\n",
    "        trainset_sizes.append(len(split[0]))\n",
    "\n",
    "    if use_unlabeled:\n",
    "        x, y = get_unlabeled(data_dir)\n",
    "        split = split_train_valid(x, y, fold=fold, folds=folds, random_state=random_state)\n",
    "        data_split = append_train_test(data_split, split)\n",
    "        trainset_sizes.append(len(split[0]))\n",
    "\n",
    "    train_x, train_y, valid_x, valid_y = data_split\n",
    "\n",
    "\n",
    "\n",
    "    train_transform = get_train_transform(augmentation = augmentation, image_size = image_size)\n",
    "    valid_transform = get_test_transform()\n",
    "\n",
    "    train_ds = TaskDataset(train_x, train_y, image_size, prep_function,\n",
    "                                      transform=train_transform,\n",
    "                                      dtype=target_dtype)\n",
    "\n",
    "    valid_ds = TaskDataset(valid_x, valid_y, image_size, prep_function,\n",
    "                                  transform=valid_transform,\n",
    "                                  dtype=target_dtype)\n",
    "\n",
    "    return train_ds, valid_ds, trainset_sizes\n",
    "\n",
    "\n",
    "def get_dataloaders(train_ds, valid_ds,\n",
    "                    batch_size,\n",
    "                    num_workers = 1,\n",
    "                    fast=False,\n",
    "                    train_sizes=None,\n",
    "                    balance=False,\n",
    "                    balance_datasets=False,\n",
    "                    balance_unlabeled=False,\n",
    "                    ):\n",
    "    sampler = None\n",
    "    weights = None\n",
    "    num_samples = 0\n",
    "\n",
    "    if balance_unlabeled:\n",
    "        labeled_mask = (train_ds.targets != UNLABELED_CLASS).astype(np.uint8)\n",
    "        weights = compute_sample_weight('balanced', labeled_mask)\n",
    "        num_samples = int(np.mean(train_sizes))\n",
    "\n",
    "    if balance:\n",
    "        weights = compute_sample_weight('balanced', train_ds.targets)\n",
    "        hist = np.bincount(train_ds.targets)\n",
    "        min_class_counts = int(min(hist))\n",
    "        num_classes = len(np.unique(train_ds.targets))\n",
    "        num_samples = min_class_counts * num_classes\n",
    "\n",
    "    if balance_datasets:\n",
    "        assert train_sizes is not None\n",
    "        dataset_balancing_term = []\n",
    "\n",
    "        for subset_size in train_sizes:\n",
    "            full_dataset_size = float(sum(train_sizes))\n",
    "            dataset_balancing_term.extend([full_dataset_size / subset_size] * subset_size)\n",
    "\n",
    "        dataset_balancing_term = np.array(dataset_balancing_term)\n",
    "        if weights is None:\n",
    "            weights = np.ones(len(train_ds.targets))\n",
    "\n",
    "        weights = weights * dataset_balancing_term\n",
    "        num_samples = int(np.mean(train_sizes))\n",
    "\n",
    "    # If we do balancing, let's go for fixed number of batches (half of dataset)\n",
    "    if weights is not None:\n",
    "        sampler = WeightedRandomSampler(weights, num_samples)\n",
    "\n",
    "    if fast:\n",
    "        weights = np.ones(len(train_ds))\n",
    "        sampler = WeightedRandomSampler(weights, 16)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size,\n",
    "                          shuffle=sampler is None, sampler=sampler,\n",
    "                          pin_memory=True, drop_last=True)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False,\n",
    "                          pin_memory=True, drop_last=False)\n",
    "\n",
    "    return train_dl, valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl import AccuracyCallback\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Невозможно создать файл, так как он уже существует: '../runs\\\\notebook_ex'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-fe82e56862f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlog_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../runs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'notebook_ex'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[1;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Невозможно создать файл, так как он уже существует: '../runs\\\\notebook_ex'"
     ]
    }
   ],
   "source": [
    "log_dir = os.path.join('../runs', 'notebook_ex')\n",
    "os.makedirs(log_dir, exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "class EfnB3(nn.Module):\n",
    "    \n",
    "    def __init__(self, middle_layer = 1024, output_layer = 5):\n",
    "        super(EfnB3, self).__init__()\n",
    "        self.efn = EfficientNet.from_name('efficientnet-b3', include_top = False)\n",
    "        self.elu = nn.ELU()\n",
    "        self.fc1 = nn.Linear(1536, middle_layer)\n",
    "        self.fc2 = nn.Linear(middle_layer, output_layer)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.efn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = EfnB3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds, train_sizes = get_datasets(data_dir='../data',\n",
    "                                                    use_current=True,\n",
    "                                                    use_extra=False,\n",
    "                                                    image_size=(300, 300),\n",
    "                                                    prep_function = 'reshape',\n",
    "                                                    augmentation='light',\n",
    "                                                    target_dtype=int,\n",
    "                                                    fold=1,\n",
    "                                                    folds=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16048"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader = get_dataloaders(train_ds, valid_ds,\n",
    "                                                batch_size=8,\n",
    "                                                train_sizes=train_sizes,\n",
    "                                                num_workers = 1,\n",
    "                                                balance=True,\n",
    "                                                balance_datasets=True,\n",
    "                                                balance_unlabeled=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 300, 300])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = next(iter(train_loader))['image']\n",
    "batch_target = next(iter(train_loader))['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "target = F.one_hot(batch_target, 5).float()\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 2, 3, 1, 1, 4, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = model(batch_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0121,  0.1245,  0.0269, -0.0268, -0.0580],\n",
       "        [ 0.0302,  0.1763, -0.0499, -0.0464,  0.0049],\n",
       "        [ 0.2470,  0.3220, -0.4684, -0.0083,  0.2111],\n",
       "        [ 0.0135,  0.1140,  0.0106, -0.0009, -0.0702],\n",
       "        [ 0.0328,  0.1236, -0.0239, -0.0121, -0.0089],\n",
       "        [ 0.0453,  0.1344, -0.1169, -0.0392,  0.0188],\n",
       "        [ 0.0134,  0.1041,  0.0158, -0.0167, -0.0525],\n",
       "        [ 0.0168,  0.1253, -0.1604, -0.0255, -0.0202]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, l = bi_tempered_logistic_loss(input, target, 0.5, 4, 0.05, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.1671, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.2337, grad_fn=<KlDivBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logp = F.log_softmax(input, dim=1)\n",
    "target_one_hot = target\n",
    "F.kl_div(logp, target_one_hot, reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from catalyst.dl import SupervisedRunner, EarlyStoppingCallback\n",
    "from catalyst.utils import load_checkpoint, unpack_checkpoint\n",
    "\n",
    "loaders = collections.OrderedDict()\n",
    "loaders[\"train\"] = train_loader\n",
    "loaders[\"valid\"] = valid_loader\n",
    "\n",
    "runner = SupervisedRunner(input_key='image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/10 * Epoch (train):   0% 0/2006 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (5) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-41d1748031e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mmain_metric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'accuracy01'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m runner.train(\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mfp16\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catalyst-20.12.1-py3.8.egg\\catalyst\\runners\\runner.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, model, criterion, optimizer, scheduler, datasets, loaders, callbacks, logdir, resume, num_epochs, valid_loader, main_metric, minimize_metric, verbose, stage_kwargs, checkpoint_data, fp16, distributed, check, overfit, timeit, load_best_on_end, initial_seed, state_kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m         \u001b[0mdistributed_cmd_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_experiment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistributed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     def infer(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catalyst-20.12.1-py3.8.egg\\catalyst\\utils\\scripts.py\u001b[0m in \u001b[0;36mdistributed_cmd_run\u001b[1;34m(worker_fn, distributed, *args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[1;32mor\u001b[0m \u001b[0mworld_size\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     ):\n\u001b[1;32m--> 178\u001b[1;33m         \u001b[0mworker_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mlocal_rank\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_rank\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catalyst-20.12.1-py3.8.egg\\catalyst\\core\\runner.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[1;34m(self, experiment)\u001b[0m\n\u001b[0;32m    827\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mException\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    828\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 829\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"on_exception\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    830\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catalyst-20.12.1-py3.8.egg\\catalyst\\core\\runner.py\u001b[0m in \u001b[0;36m_run_event\u001b[1;34m(self, event)\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m             \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_substring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"end\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catalyst-20.12.1-py3.8.egg\\catalyst\\callbacks\\exception.py\u001b[0m in \u001b[0;36mon_exception\u001b[1;34m(self, runner)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneed_exception_reraise\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catalyst-20.12.1-py3.8.egg\\catalyst\\core\\runner.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[1;34m(self, experiment)\u001b[0m\n\u001b[0;32m    824\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 826\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    827\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mException\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    828\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catalyst-20.12.1-py3.8.egg\\catalyst\\core\\runner.py\u001b[0m in \u001b[0;36m_run_experiment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    809\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"on_experiment_start\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"on_experiment_end\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catalyst-20.12.1-py3.8.egg\\catalyst\\core\\runner.py\u001b[0m in \u001b[0;36m_run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    800\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"on_stage_start\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    801\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 802\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    803\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneed_early_stop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneed_early_stop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catalyst-20.12.1-py3.8.egg\\catalyst\\core\\runner.py\u001b[0m in \u001b[0;36m_run_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    794\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"on_epoch_start\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 796\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    797\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"on_epoch_end\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catalyst-20.12.1-py3.8.egg\\catalyst\\core\\runner.py\u001b[0m in \u001b[0;36m_run_loader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    785\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_train_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader_batch_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneed_early_stop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneed_early_stop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catalyst-20.12.1-py3.8.egg\\catalyst\\core\\runner.py\u001b[0m in \u001b[0;36m_run_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"on_batch_start\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 781\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"on_batch_end\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catalyst-20.12.1-py3.8.egg\\catalyst\\core\\runner.py\u001b[0m in \u001b[0;36m_run_event\u001b[1;34m(self, event)\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m             \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_substring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"end\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catalyst-20.12.1-py3.8.egg\\catalyst\\callbacks\\metric.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, runner)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"IRunner\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[1;34m\"\"\"Computes metrics and add them to batch metrics.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_computed_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_metrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catalyst-20.12.1-py3.8.egg\\catalyst\\callbacks\\metric.py\u001b[0m in \u001b[0;36m_compute_metric_value\u001b[1;34m(self, output, input)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mmetric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetric_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-35e912efba75>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbi_tempered_logistic_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_smoothing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-35e912efba75>\u001b[0m in \u001b[0;36mbi_tempered_logistic_loss\u001b[1;34m(activations, labels, t1, t2, label_smoothing, num_iters)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[0mprobabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtempered_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m     \u001b[0mtemp1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlog_t\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlog_t\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m     \u001b[0mtemp2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[0mloss_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtemp2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (5) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import catalyst \n",
    "\n",
    "# criterions = LabelSmoothingLoss()\n",
    "criterions = TemperedLogLoss()\n",
    "# optimizer = catalyst.contrib.nn.optimizers.radam.RAdam(model.parameters(), lr = learning_rate)\n",
    "optimizer = catalyst.contrib.nn.optimizers.Adam(model.parameters(), lr = 1e-3)\n",
    "# criterions = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[25], gamma=0.8)\n",
    "# cappa = CappaScoreCallback()\n",
    "callbacks = [AccuracyCallback(num_classes=5)]\n",
    "\n",
    "\n",
    "main_metric = 'accuracy01'\n",
    "\n",
    "runner.train(\n",
    "    fp16=True,\n",
    "    model=model,\n",
    "    criterion=criterions,\n",
    "    optimizer=optimizer,\n",
    "    # scheduler=scheduler,\n",
    "    callbacks=callbacks,\n",
    "    loaders=loaders,\n",
    "    logdir=log_dir,\n",
    "    num_epochs=10,\n",
    "    verbose=1,\n",
    "    main_metric=main_metric,\n",
    "    minimize_metric=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Количество параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0f2cf5706532>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
